{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Models\n",
        "\n",
        "Train and evaluate traditional machine learning models for emotion classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import warnings\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "os.makedirs('../results/models', exist_ok=True)\n",
        "os.makedirs('../results/figures', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('../data/processed/train_processed.csv')\n",
        "val_df = pd.read_csv('../data/processed/val_processed.csv')\n",
        "test_df = pd.read_csv('../data/processed/test_processed.csv')\n",
        "\n",
        "print(f\"Train: {train_df.shape} | Val: {val_df.shape} | Test: {test_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Label Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('../data/processed/emotion_mapping.json', 'r') as f:\n",
        "    emotion_mapping = json.load(f)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_df['emotion'])\n",
        "\n",
        "print(\"Emotion to ID mapping:\")\n",
        "for emotion, idx in emotion_mapping['emotion_to_id'].items():\n",
        "    print(f\"  {emotion}: {idx}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = train_df['text'].values\n",
        "y_train = label_encoder.transform(train_df['emotion'])\n",
        "\n",
        "X_val = val_df['text'].values\n",
        "y_val = label_encoder.transform(val_df['emotion'])\n",
        "\n",
        "X_test = test_df['text'].values\n",
        "y_test = label_encoder.transform(test_df['emotion'])\n",
        "\n",
        "print(f\"Train: {len(X_train)} samples\")\n",
        "print(f\"Val: {len(X_val)} samples\")\n",
        "print(f\"Test: {len(X_test)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=2)\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
        "print(f\"Train shape: {X_train_tfidf.shape}\")\n",
        "print(f\"Val shape: {X_val_tfidf.shape}\")\n",
        "print(f\"Test shape: {X_test_tfidf.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Class Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "print(\"Class weights:\")\n",
        "for emotion, idx in emotion_mapping['emotion_to_id'].items():\n",
        "    print(f\"  {emotion:10s}: {class_weight_dict[idx]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
        "    'SVM': SVC(class_weight='balanced', random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1)\n",
        "}\n",
        "\n",
        "trained_models = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "    trained_models[name] = model\n",
        "    print(f\"  {name} trained\")\n",
        "\n",
        "print(\"\\nAll models trained\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "for name, model in trained_models.items():\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    \n",
        "    results[name] = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred, average='weighted'),\n",
        "        'recall': recall_score(y_test, y_pred, average='weighted'),\n",
        "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
        "    }\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df = results_df.round(4)\n",
        "results_df = results_df.sort_values('f1', ascending=False)\n",
        "\n",
        "print(\"Model Performance on Test Set:\")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "results_df.plot(kind='bar', y=['accuracy', 'precision', 'recall', 'f1'], ax=axes[0])\n",
        "axes[0].set_title('Model Performance Comparison', fontsize=14)\n",
        "axes[0].set_xlabel('Model')\n",
        "axes[0].set_ylabel('Score')\n",
        "axes[0].legend(loc='lower right')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "results_df['f1'].plot(kind='barh', ax=axes[1], color='steelblue')\n",
        "axes[1].set_title('F1-Score Comparison', fontsize=14)\n",
        "axes[1].set_xlabel('F1-Score')\n",
        "axes[1].set_ylabel('Model')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/figures/baseline_models_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Classification Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_name = results_df.index[0]\n",
        "best_model = trained_models[best_model_name]\n",
        "\n",
        "y_pred = best_model.predict(X_test_tfidf)\n",
        "\n",
        "print(f\"Best Model: {best_model_name}\\n\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/figures/confusion_matrix_baseline.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df.to_csv('../results/models/baseline_models_results.csv')\n",
        "\n",
        "print(\"Results saved to: ../results/models/baseline_models_results.csv\")\n",
        "print(f\"\\nBest model: {best_model_name}\")\n",
        "print(f\"Best F1-score: {results_df.loc[best_model_name, 'f1']:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
