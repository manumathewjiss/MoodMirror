{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RoBERTa Fine-tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.1\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import warnings\n",
        "from transformers import (\n",
        "    RobertaTokenizer,\n",
        "    RobertaForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "os.makedirs(\"../models/roberta_emotion_model\", exist_ok=True)\n",
        "os.makedirs(\"../results/models\", exist_ok=True)\n",
        "os.makedirs(\"../results/figures\", exist_ok=True)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (43362, 2) | Val: (5422, 2) | Test: (5423, 2)\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"../data/processed/train_processed.csv\")\n",
        "val_df = pd.read_csv(\"../data/processed/val_processed.csv\")\n",
        "test_df = pd.read_csv(\"../data/processed/test_processed.csv\")\n",
        "\n",
        "print(f\"Train: {train_df.shape} | Val: {val_df.shape} | Test: {test_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Emotion Mapping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 7\n",
            "Emotion to ID mapping:\n",
            "  anger: 0\n",
            "  disgust: 1\n",
            "  fear: 2\n",
            "  joy: 3\n",
            "  neutral: 4\n",
            "  sadness: 5\n",
            "  surprise: 6\n"
          ]
        }
      ],
      "source": [
        "with open(\"../data/processed/emotion_mapping.json\", \"r\") as f:\n",
        "    emotion_mapping = json.load(f)\n",
        "\n",
        "id_to_emotion = {int(k): v for k, v in emotion_mapping[\"id_to_emotion\"].items()}\n",
        "emotion_to_id = emotion_mapping[\"emotion_to_id\"]\n",
        "num_labels = len(emotion_to_id)\n",
        "\n",
        "print(f\"Number of classes: {num_labels}\")\n",
        "print(\"Emotion to ID mapping:\")\n",
        "for emotion, idx in emotion_to_id.items():\n",
        "    print(f\"  {emotion}: {idx}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 43362 samples\n",
            "Val: 5422 samples\n",
            "Test: 5423 samples\n"
          ]
        }
      ],
      "source": [
        "X_train = train_df[\"text\"].values\n",
        "y_train = train_df[\"emotion\"].map(emotion_to_id).values\n",
        "\n",
        "X_val = val_df[\"text\"].values\n",
        "y_val = val_df[\"emotion\"].map(emotion_to_id).values\n",
        "\n",
        "X_test = test_df[\"text\"].values\n",
        "y_test = test_df[\"emotion\"].map(emotion_to_id).values\n",
        "\n",
        "print(f\"Train: {len(X_train)} samples\")\n",
        "print(f\"Val: {len(X_val)} samples\")\n",
        "print(f\"Test: {len(X_test)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: roberta-base\n",
            "Number of parameters: 124,651,015\n"
          ]
        }
      ],
      "source": [
        "model_name = \"roberta-base\"\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    problem_type=\"single_label_classification\"\n",
        ")\n",
        "\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Number of parameters: {model.num_parameters():,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Class Balancing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original class distribution:\n",
            "  anger     :  5,335 samples\n",
            "  disgust   :    579 samples\n",
            "  fear      :    615 samples\n",
            "  joy       :  9,075 samples\n",
            "  neutral   : 23,238 samples\n",
            "  sadness   :  2,371 samples\n",
            "  surprise  :  2,149 samples\n",
            "\n",
            "Mean samples per class: 6,194\n",
            "Imbalance ratio: 40.13:1\n",
            "\n",
            "Balancing classes to mean...\n",
            "  - Oversampling classes below mean (random duplication)\n",
            "  - Undersampling classes above mean\n",
            "\n",
            "After balancing:\n",
            "  Original size: 43,362 samples\n",
            "  Balanced size: 43,358 samples\n",
            "\n",
            "Balanced class distribution (all at mean):\n",
            "  anger     :  6,194 samples\n",
            "  disgust   :  6,194 samples\n",
            "  fear      :  6,194 samples\n",
            "  joy       :  6,194 samples\n",
            "  neutral   :  6,194 samples\n",
            "  sadness   :  6,194 samples\n",
            "  surprise  :  6,194 samples\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "emotion_counts = Counter(y_train)\n",
        "print(\"Original class distribution:\")\n",
        "for emotion, idx in emotion_to_id.items():\n",
        "    count = emotion_counts[idx]\n",
        "    print(f\"  {emotion:10s}: {count:6,} samples\")\n",
        "\n",
        "mean_samples = int(np.mean(list(emotion_counts.values())))\n",
        "print(f\"\\nMean samples per class: {mean_samples:,}\")\n",
        "print(f\"Imbalance ratio: {max(emotion_counts.values()) / min(emotion_counts.values()):.2f}:1\")\n",
        "\n",
        "print(\"\\nBalancing classes to mean...\")\n",
        "print(\"  - Oversampling classes below mean (random duplication)\")\n",
        "print(\"  - Undersampling classes above mean\")\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "X_train_balanced = []\n",
        "y_train_balanced = []\n",
        "\n",
        "for emotion, idx in emotion_to_id.items():\n",
        "    class_indices = np.where(y_train == idx)[0]\n",
        "    class_count = len(class_indices)\n",
        "    \n",
        "    if class_count < mean_samples:\n",
        "        needed = mean_samples - class_count\n",
        "        oversample_indices = np.random.choice(class_indices, size=needed, replace=True)\n",
        "        selected_indices = np.concatenate([class_indices, oversample_indices])\n",
        "    elif class_count > mean_samples:\n",
        "        selected_indices = np.random.choice(class_indices, size=mean_samples, replace=False)\n",
        "    else:\n",
        "        selected_indices = class_indices\n",
        "    \n",
        "    X_train_balanced.extend(X_train[selected_indices])\n",
        "    y_train_balanced.extend(y_train[selected_indices])\n",
        "\n",
        "X_train_balanced = np.array(X_train_balanced)\n",
        "y_train_balanced = np.array(y_train_balanced)\n",
        "\n",
        "print(f\"\\nAfter balancing:\")\n",
        "print(f\"  Original size: {len(y_train):,} samples\")\n",
        "print(f\"  Balanced size: {len(y_train_balanced):,} samples\")\n",
        "\n",
        "balanced_counts = Counter(y_train_balanced)\n",
        "print(\"\\nBalanced class distribution (all at mean):\")\n",
        "for emotion, idx in emotion_to_id.items():\n",
        "    count = balanced_counts[idx]\n",
        "    print(f\"  {emotion:10s}: {count:6,} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset: 43358 samples\n",
            "Val dataset: 5422 samples\n",
            "Test dataset: 5423 samples\n"
          ]
        }
      ],
      "source": [
        "train_dataset = EmotionDataset(X_train_balanced, y_train_balanced, tokenizer)\n",
        "val_dataset = EmotionDataset(X_val, y_val, tokenizer)\n",
        "test_dataset = EmotionDataset(X_test, y_test, tokenizer)\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"Val dataset: {len(val_dataset)} samples\")\n",
        "print(f\"Test dataset: {len(test_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metrics Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    \n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision = precision_score(labels, predictions, average=\"weighted\")\n",
        "    recall = recall_score(labels, predictions, average=\"weighted\")\n",
        "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training arguments configured:\n",
            "  Epochs: 5\n",
            "  Batch size: 16\n",
            "  Learning rate: 2e-05\n",
            "  LR scheduler: SchedulerType.COSINE\n",
            "  FP16: False\n",
            "  Eval strategy: IntervalStrategy.STEPS\n",
            "  Eval steps: 500 (evaluation every 500 steps)\n",
            "  Save strategy: SaveStrategy.STEPS\n",
            "  Save steps: 500 (checkpoints saved every 500 steps)\n",
            "  Save total limit: 5 (keeps last 5 checkpoints)\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"../models/roberta_emotion_model/checkpoints\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=1000,\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_dir=\"../models/roberta_emotion_model/logs\",\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"steps\",  # Evaluate by steps (must match save_strategy when load_best_model_at_end=True)\n",
        "    eval_steps=500,  # Evaluate every 500 steps\n",
        "    save_strategy=\"steps\",  # Save checkpoints by steps (not just at epoch end)\n",
        "    save_steps=500,  # Save checkpoint every 500 steps\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=5,  # Keep last 5 checkpoints\n",
        "    seed=42,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured:\")\n",
        "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"  LR scheduler: {training_args.lr_scheduler_type}\")\n",
        "print(f\"  FP16: {training_args.fp16}\")\n",
        "print(f\"  Eval strategy: {training_args.eval_strategy}\")\n",
        "print(f\"  Eval steps: {training_args.eval_steps} (evaluation every {training_args.eval_steps} steps)\")\n",
        "print(f\"  Save strategy: {training_args.save_strategy}\")\n",
        "print(f\"  Save steps: {training_args.save_steps} (checkpoints saved every {training_args.save_steps} steps)\")\n",
        "print(f\"  Save total limit: {training_args.save_total_limit} (keeps last {training_args.save_total_limit} checkpoints)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Trainer initialized successfully\n"
          ]
        }
      ],
      "source": [
        "# Verify all required imports and variables are available\n",
        "required_imports = ['Trainer', 'TrainingArguments', 'EarlyStoppingCallback']\n",
        "missing_imports = [imp for imp in required_imports if imp not in globals()]\n",
        "if missing_imports:\n",
        "    raise ImportError(\n",
        "        f\"Missing required imports: {', '.join(missing_imports)}\\n\"\n",
        "        f\"Please run Cell 2 (Import Libraries) first to load all required modules.\"\n",
        "    )\n",
        "\n",
        "required_vars = ['model', 'training_args', 'train_dataset', 'val_dataset', 'compute_metrics']\n",
        "missing_vars = [var for var in required_vars if var not in globals()]\n",
        "if missing_vars:\n",
        "    raise NameError(\n",
        "        f\"Missing required variables: {', '.join(missing_vars)}\\n\"\n",
        "        f\"Please run the previous cells in order:\\n\"\n",
        "        f\"  - Cell 2: Import Libraries\\n\"\n",
        "        f\"  - Cell 4: Load Data\\n\"\n",
        "        f\"  - Cell 6: Load Emotion Mapping\\n\"\n",
        "        f\"  - Cell 8: Prepare Data\\n\"\n",
        "        f\"  - Cell 10: Initialize Model\\n\"\n",
        "        f\"  - Cell 12: Dataset Class\\n\"\n",
        "        f\"  - Cell 14: Class Balancing\\n\"\n",
        "        f\"  - Cell 16: Create Datasets\\n\"\n",
        "        f\"  - Cell 18: Metrics Function\\n\"\n",
        "        f\"  - Cell 20: Training Configuration\"\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "print(\"✅ Trainer initialized successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Found complete checkpoint: checkpoint-4000\n",
            "   Current progress: Epoch 1.48, Step 4000\n",
            "   Resuming training from this checkpoint...\n",
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7000' max='13550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 7000/13550 1:08:30 < 2:29:40, 0.73 it/s, Epoch 2/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.345100</td>\n",
              "      <td>1.390377</td>\n",
              "      <td>0.584286</td>\n",
              "      <td>0.684558</td>\n",
              "      <td>0.584286</td>\n",
              "      <td>0.602954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.305300</td>\n",
              "      <td>1.451694</td>\n",
              "      <td>0.591110</td>\n",
              "      <td>0.685146</td>\n",
              "      <td>0.591110</td>\n",
              "      <td>0.609686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.413300</td>\n",
              "      <td>1.257259</td>\n",
              "      <td>0.615824</td>\n",
              "      <td>0.686788</td>\n",
              "      <td>0.615824</td>\n",
              "      <td>0.632396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.443000</td>\n",
              "      <td>1.338238</td>\n",
              "      <td>0.582257</td>\n",
              "      <td>0.684225</td>\n",
              "      <td>0.582257</td>\n",
              "      <td>0.598764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.418800</td>\n",
              "      <td>1.227574</td>\n",
              "      <td>0.604021</td>\n",
              "      <td>0.678654</td>\n",
              "      <td>0.604021</td>\n",
              "      <td>0.614837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.454600</td>\n",
              "      <td>1.307538</td>\n",
              "      <td>0.579860</td>\n",
              "      <td>0.684662</td>\n",
              "      <td>0.579860</td>\n",
              "      <td>0.596186</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Training completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Check if trainer is initialized\n",
        "try:\n",
        "    _ = trainer\n",
        "except NameError:\n",
        "    raise NameError(\"Trainer not initialized. Please run Cell 22 first to initialize the trainer.\")\n",
        "\n",
        "# Check for existing checkpoints to resume from\n",
        "checkpoint_dir = \"../models/roberta_emotion_model/checkpoints\"\n",
        "resume_from_checkpoint = None\n",
        "\n",
        "def is_checkpoint_complete(checkpoint_path):\n",
        "    \"\"\"Check if a checkpoint has all required files for resuming training.\"\"\"\n",
        "    required_files = [\n",
        "        \"trainer_state.json\",\n",
        "        \"model.safetensors\",\n",
        "        \"optimizer.pt\",\n",
        "        \"scheduler.pt\",\n",
        "        \"rng_state.pth\"\n",
        "    ]\n",
        "    return all(os.path.exists(os.path.join(checkpoint_path, f)) for f in required_files)\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")]\n",
        "    if checkpoints:\n",
        "        # Sort by checkpoint number (descending)\n",
        "        checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]), reverse=True)\n",
        "        \n",
        "        # Find the latest complete checkpoint\n",
        "        complete_checkpoint = None\n",
        "        incomplete_checkpoints = []\n",
        "        \n",
        "        for checkpoint_name in checkpoints:\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
        "            if is_checkpoint_complete(checkpoint_path):\n",
        "                if complete_checkpoint is None:\n",
        "                    complete_checkpoint = checkpoint_path\n",
        "                    print(f\"✅ Found complete checkpoint: {checkpoint_name}\")\n",
        "            else:\n",
        "                incomplete_checkpoints.append(checkpoint_name)\n",
        "        \n",
        "        if complete_checkpoint:\n",
        "            resume_from_checkpoint = complete_checkpoint\n",
        "            # Load trainer state to show progress\n",
        "            import json\n",
        "            with open(os.path.join(complete_checkpoint, \"trainer_state.json\"), \"r\") as f:\n",
        "                state = json.load(f)\n",
        "            epoch = state.get(\"epoch\", 0)\n",
        "            step = state.get(\"global_step\", 0)\n",
        "            print(f\"   Current progress: Epoch {epoch:.2f}, Step {step}\")\n",
        "            print(f\"   Resuming training from this checkpoint...\")\n",
        "        else:\n",
        "            print(\"⚠️  No complete checkpoints found. Starting fresh training...\")\n",
        "        \n",
        "        # Warn about incomplete checkpoints\n",
        "        if incomplete_checkpoints:\n",
        "            print(f\"\\n⚠️  Found {len(incomplete_checkpoints)} incomplete checkpoint(s): {', '.join(incomplete_checkpoints)}\")\n",
        "            print(\"   These will be ignored. Consider cleaning them up if training completes successfully.\")\n",
        "    else:\n",
        "        print(\"ℹ️  No checkpoints found. Starting fresh training...\")\n",
        "else:\n",
        "    print(\"ℹ️  No checkpoint directory found. Starting fresh training...\")\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "try:\n",
        "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "    print(\"\\n✅ Training completed successfully!\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n⚠️  Training interrupted by user. Progress has been saved to checkpoints.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Training failed with error: {str(e)}\")\n",
        "    print(\"   You can resume from the last complete checkpoint by running this cell again.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_save_path = \"../models/roberta_emotion_model\"\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"Model saved to: {model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_results = trainer.evaluate(test_dataset)\n",
        "\n",
        "print(\"Test Set Performance:\")\n",
        "print(f\"  Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
        "print(f\"  Precision: {test_results['eval_precision']:.4f}\")\n",
        "print(f\"  Recall: {test_results['eval_recall']:.4f}\")\n",
        "print(f\"  F1-Score: {test_results['eval_f1']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "print(f\"Predictions shape: {y_pred.shape}\")\n",
        "print(f\"True labels shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "emotion_names = [id_to_emotion[i] for i in range(num_labels)]\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=emotion_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
        "            xticklabels=emotion_names,\n",
        "            yticklabels=emotion_names)\n",
        "plt.title(\"Confusion Matrix - RoBERTa\", fontsize=14)\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"../results/figures/confusion_matrix_roberta.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "roberta_results = {\n",
        "    \"accuracy\": test_results[\"eval_accuracy\"],\n",
        "    \"precision\": test_results[\"eval_precision\"],\n",
        "    \"recall\": test_results[\"eval_recall\"],\n",
        "    \"f1\": test_results[\"eval_f1\"]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame([roberta_results], index=[\"RoBERTa\"])\n",
        "results_df = results_df.round(4)\n",
        "\n",
        "print(\"RoBERTa Performance on Test Set:\")\n",
        "print(results_df)\n",
        "\n",
        "results_df.to_csv(\"../results/models/roberta_results.csv\")\n",
        "print(\"\\nResults saved to: ../results/models/roberta_results.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"RoBERTa Training Summary:\")\n",
        "print(f\"  Model: roberta-base\")\n",
        "print(f\"  Accuracy: {roberta_results['accuracy']:.4f}\")\n",
        "print(f\"  F1-Score: {roberta_results['f1']:.4f}\")\n",
        "print(f\"  Precision: {roberta_results['precision']:.4f}\")\n",
        "print(f\"  Recall: {roberta_results['recall']:.4f}\")\n",
        "print(f\"\\nModel saved to: ../models/roberta_emotion_model\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
